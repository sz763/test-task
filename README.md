# test-task

## Installation

1. Deploy postgresql with init_db.sql
2. Deploy hdfs
3. Kafka + Zookeeper
4. Deploy airflow with helm
5. Configure required variables in airflow
6. Configure airflow gitSync with this repository and set branch: main, subPath `airflow/dags`
7. Build project and put shaded jar to /opt/jars/
8. Unpause the DAG test-task-spark-app in Airflow

>Как будете партицировать данные?

hdfs: Данные партицируются по timestamp попадания в kafka
kafka: Партиционировать по году и номеру недели (в зависимости от объема данных при условии что timestamp существует в записи)


> Что будете мониторить и как?
1. Кафка
    * Количество записаных/прочтенных данных
    * Состояние реплик
    * Оффсеты кафки
    * Состояние партиций (всего партиций, оффлайн партиций)
    * Lag (оффсет кафки - прочитаный оффсет спарк джобой per partition)
    * ЦПУ
    * Диск
2. DB
    * ЦПУ
    * Диск
    * Активные соединения
3. HDFS
    * Сколько данных хранится (Сколько инфы о данных в NameNode, сколько хранится данных в DataNode)
    * Капасити
    * Активны ноды
    * Сдохшие ноды
    * JVM

> Что будете логгировать?

* Начало/завершение работы приложения
* Количество прочтенных записей
* Количество дублей
* Количество невалидных данных
* Ошибки

> Что будете делать с оффсетом?

Оффсет для каждой партиции сохраняется в базу по завершении процессинга, каждый последующий запуск использует в качестве startOffset - предыдущий endOffset. Так же есть возможность задать batchSize для ограничения чтения данных, к примеру для historical load.

> Сериализация данных?

json читается в dataframe и преобразуется в struct, вся работа проходит вне хипа, для увеличения производительности. Данные сохраняются в формате parquet, т.к. исходя из постановки задачи не ожидается Schema Evlution, а Spark не поддерживает схему On- Read.  
